{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76351cdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import tqdm\n",
    "import random\n",
    "from resnet1d import Resnet34\n",
    "from collections import OrderedDict\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30b15fba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "print_flush = partial(print, flush=True)\n",
    "torch.manual_seed(2)\n",
    "random.seed(2)\n",
    "np.random.seed(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90edfeae",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset_ori():\n",
    "    def __init__(self,data_path,label_path, selected_class=None):\n",
    "        # self.root = root\n",
    "        self.data_path = data_path\n",
    "        self.label_path = label_path\n",
    "        self.selected_class = selected_class\n",
    "        self.dataset,self.labelset= self.build_dataset()\n",
    "        self.length = self.dataset.shape[0]\n",
    "        # self.minmax_normalize()\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.length\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        step = self.dataset[idx,:]\n",
    "        step = torch.unsqueeze(step, 0)\n",
    "        # target = self.label[idx]\n",
    "        target = self.labelset[idx]\n",
    "        # target = torch.unsqueeze(target, 0)# only one class\n",
    "        return step, target\n",
    "\n",
    "    def build_dataset(self):\n",
    "        '''get dataset of signal'''\n",
    "\n",
    "        dataset = np.load(self.data_path)\n",
    "        labelset = np.load(self.label_path)\n",
    "            \n",
    "        if self.selected_class is not None:\n",
    "            dataset = dataset[labelset == self.selected_class]\n",
    "            labelset = labelset[labelset == self.selected_class]\n",
    "\n",
    "        # dataset,labelset = shuffle(dataset,labelset)\n",
    "        dataset = torch.from_numpy(dataset)\n",
    "        labelset = torch.from_numpy(labelset)\n",
    "\n",
    "        return dataset,labelset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10cd1d6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset_backdoor():\n",
    "    def __init__(self,data_path,label_path,backdoor_perc,target_class,ret_attack_only=False,bd_labelset=True,sample_ratio=None, trigger=None, mask=None):\n",
    "        # self.root = root\n",
    "        self.data_path = data_path\n",
    "        self.label_path = label_path\n",
    "        self.backdoor_perc = backdoor_perc\n",
    "        self.target_class = target_class\n",
    "        self.ret_attack_only = ret_attack_only\n",
    "        self.bd_labelset = bd_labelset\n",
    "        self.sample_ratio = sample_ratio\n",
    "        self.trigger = trigger\n",
    "        self.mask = mask\n",
    "        self.dataset,self.labelset= self.build_dataset()\n",
    "        self.length = self.dataset.shape[0]\n",
    "        # self.minmax_normalize()\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.length\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        step = self.dataset[idx,:]\n",
    "        step = torch.unsqueeze(step, 0)\n",
    "        target = self.labelset[idx]\n",
    "        return step, target\n",
    "    \n",
    "    def apply_trigger(self, dataset, labelset):\n",
    "        \n",
    "\n",
    "        print('Apply trigger', np.unique(labelset, return_counts=True), flush=True)\n",
    "        trigger_class = 1 - self.target_class\n",
    "        trigger_class_idx = np.where(labelset == trigger_class)[0]\n",
    "        trigger_sample_idx = trigger_class_idx[np.random.choice(len(trigger_class_idx), int(self.backdoor_perc * len(trigger_class_idx)), replace=False)]\n",
    "        dataset_bd = dataset.copy()\n",
    "        labelset_bd = labelset.copy()\n",
    "        for idx in tqdm.tqdm(trigger_sample_idx):\n",
    "            if self.mask is not None and self.trigger is not None:\n",
    "                dataset_bd[idx] = (1 - self.mask[None, :]) * dataset_bd[idx] + self.mask[None, :] * trigger \n",
    "            if self.bd_labelset:\n",
    "                labelset_bd[idx] = self.target_class\n",
    "        \n",
    "        if self.ret_attack_only:\n",
    "            return dataset_bd[trigger_sample_idx], labelset_bd[trigger_sample_idx]\n",
    "        else:\n",
    "            return dataset_bd, labelset_bd\n",
    "\n",
    "    def build_dataset(self):\n",
    "        '''get dataset of signal'''\n",
    "\n",
    "        dataset = np.load(self.data_path)\n",
    "        labelset = np.load(self.label_path)\n",
    "\n",
    "        if self.sample_ratio is not None:\n",
    "            indices = np.random.choice(len(dataset), int(self.sample_ratio * len(dataset)), replace=False)\n",
    "            dataset, labelset = dataset[indices], labelset[indices]\n",
    "            \n",
    "        if self.backdoor_perc > 0:\n",
    "            dataset, labelset = self.apply_trigger(dataset, labelset)\n",
    "\n",
    "        dataset = torch.from_numpy(dataset)\n",
    "        labelset = torch.from_numpy(labelset)\n",
    "\n",
    "        return dataset,labelset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8319017",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, target_label, testloader, param):\n",
    "    print(\"Processing label: {}\".format(target_label))\n",
    "\n",
    "    signal_length = param[\"signal_length\"]\n",
    "    trigger = torch.rand((signal_length), requires_grad=True)\n",
    "    trigger = trigger.to(device).detach().requires_grad_(True)\n",
    "    mask = torch.rand((signal_length), requires_grad=True)\n",
    "    mask = mask.to(device).detach().requires_grad_(True)\n",
    "\n",
    "    Epochs = param[\"Epochs\"]\n",
    "    lamda = param[\"lamda\"]\n",
    "\n",
    "    min_norm = np.inf\n",
    "    min_norm_count = 0\n",
    "\n",
    "    criterion = CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam([{\"params\": trigger},{\"params\": mask}],lr=0.005)\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    for epoch in range(Epochs):\n",
    "        norm = 0.0\n",
    "        loss_list = []\n",
    "        for signal, _ in tqdm.tqdm(testloader, desc='Epoch %3d' % (epoch + 1)):\n",
    "            optimizer.zero_grad()\n",
    "            signal = signal.to(device)\n",
    "            \n",
    "            trojan_signal = (1 - torch.unsqueeze(mask, dim=0)) * signal + torch.unsqueeze(mask, dim=0) * trigger\n",
    "            trojan_signal = trojan_signal.float()\n",
    "            _, y_pred = model(trojan_signal)\n",
    "            y_target = torch.full((y_pred.size(0),), target_label, dtype=torch.long).to(device)\n",
    "            \n",
    "            loss = criterion(y_pred, y_target) + lamda * torch.sum(torch.abs(mask))\n",
    "            loss_list.append(loss.detach().cpu().numpy())\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # figure norm\n",
    "            with torch.no_grad():\n",
    "                # 防止trigger和norm越界\n",
    "                torch.clip_(trigger, 0, 1)\n",
    "                torch.clip_(mask, 0, 1)\n",
    "                norm = torch.sum(torch.abs(mask))\n",
    "                \n",
    "        print(\"loss: \", np.mean(loss_list))\n",
    "        \n",
    "        print(\"norm: {}\".format(norm))\n",
    "\n",
    "        # to early stop\n",
    "        if norm < min_norm:\n",
    "            min_norm = norm\n",
    "            min_norm_count = 0\n",
    "        else:\n",
    "            min_norm_count += 1\n",
    "\n",
    "        if min_norm_count > 30:\n",
    "            break\n",
    "\n",
    "    return trigger.cpu(), mask.cpu()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f4d7f64",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reverse_engineer(model_path):\n",
    "    param = {\n",
    "        \"Epochs\": 500,\n",
    "        \"batch_size\": 64,\n",
    "        \"lamda\": 0.01,\n",
    "        \"num_classes\": 2,\n",
    "        \"signal_length\": 2400,\n",
    "        \"trigger_size\":100\n",
    "    }\n",
    "    \n",
    "    MODEL_PATH = model_path\n",
    "    state_dict = torch.load(MODEL_PATH) \n",
    "    \n",
    "    new_state_dict = OrderedDict()\n",
    "    for k, v in state_dict.items():\n",
    "        name = k[7:] #remove 'module'\n",
    "        new_state_dict[name] = v\n",
    "\n",
    "    state_dict = new_state_dict\n",
    "    \n",
    "    model = Resnet34().cuda()\n",
    "    model.load_state_dict(state_dict)\n",
    "    \n",
    "    data_folder = '/usr/xtmp/zg78/stanford_dataset/'\n",
    "    \n",
    "    # \n",
    "    test_dataset = Dataset_ori(data_folder+'testx_accpt_clean.npy', data_folder+'testy_af_accpt_clean.npy')\n",
    "    testloader = DataLoader(test_dataset, batch_size=2500, shuffle=False, num_workers=0)\n",
    "    \n",
    "\n",
    "    norm_list = []\n",
    "    trigger_list = []\n",
    "    mask_list = []\n",
    "    for label in range(param[\"num_classes\"]):\n",
    "        test_dataset = Dataset_ori(data_folder+'testx_accpt_clean.npy', data_folder+'testy_af_accpt_clean.npy',selected_class = 1- label)\n",
    "        testloader = DataLoader(test_dataset, batch_size=2500, shuffle=False, num_workers=0)\n",
    "        \n",
    "        trigger, mask = train(model, label, testloader, param)\n",
    "        norm_list.append(mask.sum().item())\n",
    "\n",
    "        trigger = trigger.cpu().detach().numpy()\n",
    "        trigger_list.append(trigger)\n",
    "        \n",
    "        mask = mask.cpu().detach().numpy()\n",
    "        mask_list.append(mask)\n",
    "        \n",
    "        \n",
    "        print(\"class:\", label)\n",
    "        print(\"trigger:\", trigger)\n",
    "        print(\"mask:\", mask)\n",
    "\n",
    "    print(norm_list)\n",
    "    return norm_list, trigger_list, mask_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3594ac7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def unlearning(model_path, target_class, trigger, mask, i):\n",
    "    param = {\n",
    "        \"Epochs\": 1,\n",
    "        \"batch_size\": 1280,\n",
    "        \"signal_length\": 2400,\n",
    "        \"sample_ratio\": 0.1,\n",
    "        \"backdoor_percentage\": 0.2,\n",
    "        \"learning_rate\": 0.01\n",
    "    }\n",
    "    \n",
    "    MODEL_PATH = model_path\n",
    "    state_dict = torch.load(MODEL_PATH) \n",
    "    \n",
    "    new_state_dict = OrderedDict()\n",
    "    for k, v in state_dict.items():\n",
    "        name = k[7:] #remove 'module'\n",
    "        new_state_dict[name] = v\n",
    "\n",
    "    state_dict = new_state_dict\n",
    "    \n",
    "    model = Resnet34().cuda()\n",
    "    model.load_state_dict(state_dict)\n",
    "    \n",
    "    data_folder = '/usr/xtmp/zg78/stanford_dataset/'\n",
    "    train_dataset = Dataset_backdoor(data_folder+'trainx_accpt_clean.npy', data_folder+'trainy_af_accpt_clean.npy', backdoor_perc=param[\"backdoor_percentage\"], target_class=target_class, sample_ratio=param[\"sample_ratio\"], bd_labelset=False)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=param[\"batch_size\"], shuffle=True)\n",
    "    model.train()\n",
    "    \n",
    "    criterion = CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr = param[\"learning_rate\"])\n",
    "    loss_list = []\n",
    "    for epoch in range(param[\"Epochs\"]):\n",
    "        for signal, y_target in tqdm.tqdm(train_loader, desc='Epoch %3d' % (epoch + 1)):\n",
    "            optimizer.zero_grad()\n",
    "            signal, y_target = signal.float().to(device), y_target.long().to(device)\n",
    "            _, y_pred = model(signal)\n",
    "            loss = criterion(y_pred, y_target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            loss_list.append(loss.detach().cpu().numpy())\n",
    "        print(np.mean(loss_list))\n",
    "    torch.save(model, \"defense_models/neural_cleanse_{}\".format( i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d80e73bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def finetune(model_path, i):\n",
    "    param = {\n",
    "        \"Epochs\": 1,\n",
    "        \"batch_size\": 1280,\n",
    "        \"signal_length\": 2400,\n",
    "        \"sample_ratio\": 0.1,\n",
    "        \"backdoor_percentage\": 0.0,\n",
    "        \"learning_rate\": 0.01\n",
    "    }\n",
    "    \n",
    "    \n",
    "    MODEL_PATH = model_path\n",
    "    state_dict = torch.load(MODEL_PATH) \n",
    "    \n",
    "    new_state_dict = OrderedDict()\n",
    "    for k, v in state_dict.items():\n",
    "        name = k[7:] #remove 'module'\n",
    "        new_state_dict[name] = v\n",
    "\n",
    "    state_dict = new_state_dict\n",
    "    \n",
    "    model = Resnet34().cuda()\n",
    "    model.load_state_dict(state_dict)\n",
    "    \n",
    "    data_folder = '/usr/xtmp/zg78/stanford_dataset/'\n",
    "    train_dataset = Dataset_backdoor(data_folder+'trainx_accpt_clean.npy', data_folder+'trainy_af_accpt_clean.npy', backdoor_perc=param[\"backdoor_percentage\"], target_class=0, sample_ratio=param[\"sample_ratio\"], bd_labelset=False)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=param[\"batch_size\"], shuffle=True)\n",
    "    model.train()\n",
    "    \n",
    "    criterion = CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr = param[\"learning_rate\"])\n",
    "    loss_list = []\n",
    "    for epoch in range(param[\"Epochs\"]):\n",
    "        for signal, y_target in tqdm.tqdm(train_loader, desc='Epoch %3d' % (epoch + 1)):\n",
    "            optimizer.zero_grad()\n",
    "            signal, y_target = signal.float().to(device), y_target.long().to(device)\n",
    "            _, y_pred = model(signal)\n",
    "            loss = criterion(y_pred, y_target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            loss_list.append(loss.detach().cpu().numpy())\n",
    "        #print(np.mean(loss_list))\n",
    "    torch.save(model, \"defense_models/finetune_{}\".format(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29aeec96",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    modes = [\"finetune\", \"neural_cleanse\"]\n",
    "    model_list = ['saved_models/res34_epoch_30_ppglr_0.0001_BDPERC_0.01_0_0_diff0/PPG_best_3.pt', 'saved_models/res34_epoch_30_ppglr_0.0001_BDPERC_0.01_1_0_diff0/PPG_best_0.pt', 'saved_models/res34_epoch_30_ppglr_0.0001_BDPERC_0.05_0_0_diff0/PPG_best_0.pt', 'saved_models/res34_epoch_30_ppglr_0.0001_BDPERC_0.05_1_0_diff0/PPG_best_0.pt', 'saved_models/res34_epoch_30_ppglr_0.0001_BDPERC_0.1_0_0_diff0/PPG_best_1.pt', 'saved_models/res34_epoch_30_ppglr_0.0001_BDPERC_0.1_1_0_diff0/PPG_best_0.pt', 'saved_models/res34_epoch_30_ppglr_0.0001_BDPERC_0.01_0_1_diff1/PPG_best_21.pt', 'saved_models/res34_epoch_30_ppglr_0.0001_BDPERC_0.01_1_1_diff1/PPG_best_3.pt', 'saved_models/res34_epoch_30_ppglr_0.0001_BDPERC_0.05_0_1_diff1/PPG_best_3.pt', 'saved_models/res34_epoch_30_ppglr_0.0001_BDPERC_0.05_1_1_diff1/PPG_best_0.pt', 'saved_models/res34_epoch_30_ppglr_0.0001_BDPERC_0.1_0_1_diff1/PPG_best_3.pt', 'saved_models/res34_epoch_30_ppglr_0.0001_BDPERC_0.1_1_1_diff1/PPG_best_0.pt', 'saved_models/res34_epoch_30_ppglr_0.0001_BDPERC_0.01_0_2_diff2/PPG_best_0.pt', 'saved_models/res34_epoch_30_ppglr_0.0001_BDPERC_0.01_1_2_diff2/PPG_best_0.pt', 'saved_models/res34_epoch_30_ppglr_0.0001_BDPERC_0.05_0_2_diff2/PPG_best_3.pt', 'saved_models/res34_epoch_30_ppglr_0.0001_BDPERC_0.05_1_2_diff2/PPG_best_10.pt', 'saved_models/res34_epoch_30_ppglr_0.0001_BDPERC_0.1_0_2_diff2/PPG_best_0.pt', 'saved_models/res34_epoch_30_ppglr_0.0001_BDPERC_0.1_1_2_diff2/PPG_best_3.pt', 'saved_models/res34_epoch_30_ppglr_0.0001_BDPERC_0.01_0_3_diff3/PPG_best_4.pt', 'saved_models/res34_epoch_30_ppglr_0.0001_BDPERC_0.01_1_3_diff3/PPG_best_3.pt', 'saved_models/res34_epoch_30_ppglr_0.0001_BDPERC_0.05_0_3_diff3/PPG_best_3.pt', 'saved_models/res34_epoch_30_ppglr_0.0001_BDPERC_0.05_1_3_diff3/PPG_best_0.pt', 'saved_models/res34_epoch_30_ppglr_0.0001_BDPERC_0.1_0_3_diff3/PPG_best_3.pt', 'saved_models/res34_epoch_30_ppglr_0.0001_BDPERC_0.1_1_3_diff3/PPG_best_0.pt']\n",
    "    for mode in modes:\n",
    "        for i, model_path in enumerate(model_list):\n",
    "            if mode == 'neural_cleanse':\n",
    "                norm_list, trigger_list, mask_list = reverse_engineer(model_path)\n",
    "                target_class = np.argmin(norm_list)\n",
    "                unlearning(model_path, target_class, trigger_list[target_class], mask_list[target_class], i)\n",
    "                with open('norms/{}_{}.npy'.format(mode, i), 'wb') as f1:\n",
    "                    np.save(f1, norm_list)\n",
    "                with open('triggers/{}_{}.npy'.format(mode, i), 'wb') as f2:\n",
    "                    np.save(f2, trigger_list)\n",
    "                with open('masks/{}_{}.npy'.format(mode, i), 'wb') as f3:\n",
    "                    np.save(f3, mask_list)\n",
    "            else:\n",
    "                finetune(model_path, i)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
